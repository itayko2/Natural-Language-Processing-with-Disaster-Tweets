{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-07T21:03:35.351138Z","iopub.execute_input":"2022-05-07T21:03:35.351391Z","iopub.status.idle":"2022-05-07T21:03:35.360685Z","shell.execute_reply.started":"2022-05-07T21:03:35.351362Z","shell.execute_reply":"2022-05-07T21:03:35.359689Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\n# Get the GPU device name.\ndevice_name = tf.test.gpu_device_name()\n\n# The device name should look like the following:\nif device_name == '/device:GPU:0':\n    print('Found GPU at: {}'.format(device_name))\nelse:\n    raise SystemError('GPU device not found')","metadata":{"execution":{"iopub.status.busy":"2022-05-07T21:02:50.809560Z","iopub.execute_input":"2022-05-07T21:02:50.809862Z","iopub.status.idle":"2022-05-07T21:02:55.892196Z","shell.execute_reply.started":"2022-05-07T21:02:50.809824Z","shell.execute_reply":"2022-05-07T21:02:55.890760Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import torch\n\n# If there's a GPU available...\nif torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-05-07T21:03:23.656175Z","iopub.execute_input":"2022-05-07T21:03:23.656458Z","iopub.status.idle":"2022-05-07T21:03:25.271513Z","shell.execute_reply.started":"2022-05-07T21:03:23.656384Z","shell.execute_reply":"2022-05-07T21:03:25.270784Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-05-07T21:03:40.478836Z","iopub.execute_input":"2022-05-07T21:03:40.479476Z","iopub.status.idle":"2022-05-07T21:03:40.538869Z","shell.execute_reply.started":"2022-05-07T21:03:40.479437Z","shell.execute_reply":"2022-05-07T21:03:40.538168Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_df.columns","metadata":{"execution":{"iopub.status.busy":"2022-05-07T10:11:59.369486Z","iopub.execute_input":"2022-05-07T10:11:59.369777Z","iopub.status.idle":"2022-05-07T10:11:59.377066Z","shell.execute_reply.started":"2022-05-07T10:11:59.369747Z","shell.execute_reply":"2022-05-07T10:11:59.375901Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Preprocess text (username and link placeholders)\ndef preprocess(text):\n    new_text = []\n\n    for t in text.split(\" \"):\n        t = '' if t.startswith('@') and len(t) > 1 else t\n        t = '' if t.startswith('http') else t\n        t = t.replace(\"#\",\"\")\n        new_text.append(t.lower())\n    return \" \".join(new_text).strip().replace(\"  \", \" \")","metadata":{"execution":{"iopub.status.busy":"2022-05-07T21:03:44.712890Z","iopub.execute_input":"2022-05-07T21:03:44.713465Z","iopub.status.idle":"2022-05-07T21:03:44.718837Z","shell.execute_reply.started":"2022-05-07T21:03:44.713424Z","shell.execute_reply":"2022-05-07T21:03:44.717887Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_df[\"text\"] = train_df[\"text\"].apply(lambda x: preprocess(x))\ntest_df[\"text\"] = test_df[\"text\"].apply(lambda x: preprocess(x))","metadata":{"execution":{"iopub.status.busy":"2022-05-07T21:03:46.525553Z","iopub.execute_input":"2022-05-07T21:03:46.526141Z","iopub.status.idle":"2022-05-07T21:03:46.652083Z","shell.execute_reply.started":"2022-05-07T21:03:46.526105Z","shell.execute_reply":"2022-05-07T21:03:46.651397Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(train_df[\"text\"], train_df[\"target\"], test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T21:03:48.266512Z","iopub.execute_input":"2022-05-07T21:03:48.267008Z","iopub.status.idle":"2022-05-07T21:03:49.044057Z","shell.execute_reply.started":"2022-05-07T21:03:48.266971Z","shell.execute_reply":"2022-05-07T21:03:49.043341Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_labels.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T21:03:52.730500Z","iopub.execute_input":"2022-05-07T21:03:52.731196Z","iopub.status.idle":"2022-05-07T21:03:52.739586Z","shell.execute_reply.started":"2022-05-07T21:03:52.731161Z","shell.execute_reply":"2022-05-07T21:03:52.738282Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\nmodel = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T21:04:00.670854Z","iopub.execute_input":"2022-05-07T21:04:00.671231Z","iopub.status.idle":"2022-05-07T21:04:16.292756Z","shell.execute_reply.started":"2022-05-07T21:04:00.671188Z","shell.execute_reply":"2022-05-07T21:04:16.291993Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\nval_encodings = tokenizer(list(val_texts), truncation=True, padding=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T21:04:33.705728Z","iopub.execute_input":"2022-05-07T21:04:33.706178Z","iopub.status.idle":"2022-05-07T21:04:34.337130Z","shell.execute_reply.started":"2022-05-07T21:04:33.706143Z","shell.execute_reply":"2022-05-07T21:04:34.336377Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Create torch dataset\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, text, labels=None):\n        self.text = text\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.text.items()}\n        if self.labels:\n            item[\"labels\"] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.text[\"input_ids\"])\n\ntrain_dataset = Dataset(train_encodings, list(train_labels))\nval_dataset = Dataset(val_encodings, list(val_labels))","metadata":{"execution":{"iopub.status.busy":"2022-05-07T21:04:42.480388Z","iopub.execute_input":"2022-05-07T21:04:42.480714Z","iopub.status.idle":"2022-05-07T21:04:42.490361Z","shell.execute_reply.started":"2022-05-07T21:04:42.480684Z","shell.execute_reply":"2022-05-07T21:04:42.489311Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\nimport torch\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import EarlyStoppingCallback\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-07T21:04:51.660743Z","iopub.execute_input":"2022-05-07T21:04:51.660991Z","iopub.status.idle":"2022-05-07T21:04:51.830942Z","shell.execute_reply.started":"2022-05-07T21:04:51.660963Z","shell.execute_reply":"2022-05-07T21:04:51.830199Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# ----- 2. Fine-tune pretrained model -----#\n# Define Trainer parameters\ndef compute_metrics(p):\n    pred, labels = p\n    pred = np.argmax(pred, axis=1)\n    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n    return {\"accuracy\": accuracy}\n\n# Define Trainer\nargs = TrainingArguments(\n    output_dir=\"output\",\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=1,\n    seed=0,\n    load_best_model_at_end=True,\n)\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n)\n\n# Train our pre-trained BERT model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:30:01.494649Z","iopub.execute_input":"2022-05-07T17:30:01.495519Z","iopub.status.idle":"2022-05-07T17:30:47.938310Z","shell.execute_reply.started":"2022-05-07T17:30:01.495467Z","shell.execute_reply":"2022-05-07T17:30:47.937470Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ['WANDB_DISABLED'] = 'true'","metadata":{"execution":{"iopub.status.busy":"2022-05-07T21:09:50.723535Z","iopub.execute_input":"2022-05-07T21:09:50.724076Z","iopub.status.idle":"2022-05-07T21:09:50.727890Z","shell.execute_reply.started":"2022-05-07T21:09:50.724040Z","shell.execute_reply":"2022-05-07T21:09:50.726995Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='./results',          # output directory\n    num_train_epochs=3,              # total number of training epochs\n    per_device_train_batch_size=16,  # batch size per device during training\n    per_device_eval_batch_size=64,   # batch size for evaluation\n    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir='./logs',            # directory for storing logs\n    logging_steps=10,\n)\n\ntrainer = Trainer(\n    model=model,                         # the instantiated 🤗 Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=val_dataset             # evaluation dataset\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T21:09:53.288708Z","iopub.execute_input":"2022-05-07T21:09:53.289429Z","iopub.status.idle":"2022-05-07T21:11:31.824590Z","shell.execute_reply.started":"2022-05-07T21:09:53.289392Z","shell.execute_reply":"2022-05-07T21:11:31.823871Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# ----- 3. Predict -----#\ntest_encodings = tokenizer(list(test_df[\"text\"]), padding=True, truncation=True, max_length=512)\n\n# Create torch dataset\ntest_dataset = Dataset(test_encodings)\n\n# Define test trainer\ntest_trainer = Trainer(model)\n\n# Make prediction\nraw_pred, _, _ = test_trainer.predict(test_dataset)\n\n# Preprocess raw predictions\ny_pred = np.argmax(raw_pred, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T21:13:29.400195Z","iopub.execute_input":"2022-05-07T21:13:29.400474Z","iopub.status.idle":"2022-05-07T21:13:34.087356Z","shell.execute_reply.started":"2022-05-07T21:13:29.400446Z","shell.execute_reply":"2022-05-07T21:13:34.086106Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsample_submission[\"target\"] = y_pred\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-07T21:13:40.909377Z","iopub.execute_input":"2022-05-07T21:13:40.909650Z","iopub.status.idle":"2022-05-07T21:13:40.922110Z","shell.execute_reply.started":"2022-05-07T21:13:40.909607Z","shell.execute_reply":"2022-05-07T21:13:40.921442Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"sample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T21:13:44.192406Z","iopub.execute_input":"2022-05-07T21:13:44.192675Z","iopub.status.idle":"2022-05-07T21:13:44.204824Z","shell.execute_reply.started":"2022-05-07T21:13:44.192627Z","shell.execute_reply":"2022-05-07T21:13:44.203813Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"sample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T21:13:46.512456Z","iopub.execute_input":"2022-05-07T21:13:46.512727Z","iopub.status.idle":"2022-05-07T21:13:46.525084Z","shell.execute_reply.started":"2022-05-07T21:13:46.512698Z","shell.execute_reply":"2022-05-07T21:13:46.524419Z"},"trusted":true},"execution_count":27,"outputs":[]}]}